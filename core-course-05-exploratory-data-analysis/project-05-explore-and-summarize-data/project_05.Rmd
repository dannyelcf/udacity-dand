---
output: github_document
---
EDA in an Issue Tracking Data Set
================================================================================
*by Dannyel Cardoso da Fonseca* 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Cleaning the environment before start
# This is useful when execute "Restart R and Run All Chuncks"
rm(list = ls())
```

```{r echo=FALSE, message=FALSE, warning=FALSE, "Load Packages"}
library(ggplot2)
library(dplyr)
library(lubridate)
library(grid)

source("project_05.R")
```

```{r echo=FALSE, message=FALSE, warning=FALSE, "Set Up"}
# NOTE: To run this notebook set up current work directory to this file location

# Set up default theme for plots
theme_set(theme_classic() %+replace%
            theme(panel.grid.major.y = element_line(size=.05, color="gray"),
                  plot.margin = unit(c(1, .5, 1, 2), "cm")))
```

```{r echo=FALSE, "Load Data Set"}
issues <- load_dataset("issues_tracking.csv")
```

This project aims to explore a data set containing 22,125 observations of issues 
tracking and their history logs. The data set represents 5 years of project 
management which aimed maintenance and customization of many integrated systems
([SIG](https://docs.info.ufrn.br)) for the academic, administrative and human 
resources management at the Universidade Federal de GoiÃ¡s ([UFG](https://www.ufg.br)).

A proprietary issue tracking system ([SIGProject](https://sigproject.esig.com.br)) 
was used to manage activities of teams.

![Screenshot](project_05_files/screenshot-sigproject.esig.com.br-2018.02.25-11-25-48.png)

I used data wrangling techniques to export data from that and clean them. The 
final data set, used in this project, and its documentation can be accessed, 
respectively, in these links: 

- [Issues Data Set](issues_tracking.csv)
- [Issues Data Set Documentation](issues_tracking.Rdoc.txt)

The exported data set, the wrangling process scripts and an example of one issue 
tracking can be find in [data_wrangling](data_wrangling) folder. 

# Data Set Summaries

In this section I perform some preliminary exploration on the data set, analyzing 
its structure and the distributions of some variables.

### Data Set Structure

The issues data set contains 22,125 rows and 24 variables. Of these 24 variables 
16 are about issue data and 8 about issue's logs.

```{r echo=FALSE, message=FALSE, warning=FALSE, "Structure of the Data Set"}
# Show the issues' dataframe structure
str(issues, max.level=1, vec.len=1, nchar.max=50)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, "Support Dataframe to Plot (1)"}
# From issues dataframe, it was created another dataframe which counts number of 
# logs per issue_id
df.number_logs_per_issue <- 
  issues %>%
    select(issue_id, log_creation_date) %>%
    group_by(issue_id) %>% 
    summarise(score = sum(!is.na(log_creation_date))) %>% 
    as.data.frame()
```

The number of distinct issues rows and issue's logs rows are 4,503 and 21,978
respectively.

```{r echo=FALSE, message=FALSE, warning=FALSE, "Number of issues and issue's logs"}
data.frame(number_issues = nrow(df.number_logs_per_issue), 
           number_logs = sum(df.number_logs_per_issue$score))
```
We can note that the number of issues represents 20% of the data set and the 
number of logs represents 99% of it. The 1% of logs remaining (147 rows) represents 
issues that do not have history logs. The distribution of number of logs per issue 
and their summaries are presented in the barplot below.

```{r echo=FALSE, fig.retina=2, "Distribution of Number of Logs per Issue"}
# Count number of issues for each number of logs per issue
df.number_logs_per_issue %>%
  group_by(number_logs_per_issue = score) %>%
  summarise(score = n()) %>% 
  plot_frequency.numeric(x = number_logs_per_issue, y = score,
                         summary = df_summary(df.number_logs_per_issue, score),
                         x.breaks = seq(0, 43, 2), y.breaks = seq(0, 800, 50),
                         labs.title = "Distribution of Number of Logs per Issue", 
                         labs.x = "Number of logs per issue")
```

Analysing the plot above, we can note that 75% of issues have up to 6 history logs
(before 3rd quartile) and 50% of issues have between 2 and 6 logs (between 1st 
and 3rd quartile). Besides the skewed shape of plot, the median and mean are very 
close. This means that the amount of ouliers, after 12 logs per issue, is low, 
approximately 5% of issues.  

### Issue Creation Date

This data set comprises issues created in the period between 05/21/2013 and 
01/26/2018.

```{r echo=FALSE, message=FALSE, warning=FALSE, "Start and End Issue's Creation Date"}
data.frame(first_issue_creation_date = format(min(issues$issue_creation_date), 
                                              "%m/%d/%Y %H:%M:%S"), 
           last_issue_creation_date = format(max(issues$issue_creation_date), 
                                              "%m/%d/%Y %H:%M:%S"))
```

The distribution of number of issues created by month and the cumulative summaries 
are presented in the barplot below. 

```{r echo=FALSE, fig.retina=2, "Distribution of Number of Issues Created per Month"}
issues %>% 
  distinct(issue_id, issue_creation_date = as.Date(floor_date(issue_creation_date, 
                                                              "month"))) %>% 
  group_by(issue_creation_date) %>% 
  summarise(score = n()) %>% 
  plot_frequency.month(x = issue_creation_date, y = score,
                      y.breaks = seq(0, 190, 10),
                      labs.title = "Distribution of Number of Issues Created per Month", 
                      labs.x = "Issue creation date (per month)")
  
```

Analysing the plot above, we note that the year 2013 had the lowest demand. The
accumulated mean (red solid line) variates between 10 and 20 issues per month. At 
the end of 2013 and begin of 2014 there was a drop in activities because of the 
low administrative and academic activities at the university in this period. From
April of 2014 until February of 2015 there was significant growth in activities.
The accumulated mean increases from 10 to 70 issues per month. The accumulated mean
overcame the accumulated median and the distance between accumulated 3rd quartile 
and accumulated median became greater than distance between accumulated 1st 
quartile and accumulated median. This means that theses months had a high number 
of issues created in relation to the past. 

But from August of 2015 the pace of growth dropped. Now, we can note that the 
distance between accumulated 1st quartile and accumulated median becomes greater 
than distance between accumulated 3rd quartile and accumulated median. This 
invertion provocated the overlap between accumulated mean and accumulated median. 
The stabilization of growth also contributed to that. The average of growth pass 
to be only 10 monthly issues from February 2015. 

We also note that except the significant growth period (April of 2014 until 
February of 2015) the months from September to December of each year had a 
decline in activities. The reason is the low administrative and academic 
activities at the university in this period. Thus, the demand for the systems 
decreases.

### Issue Start Date

```{r echo=FALSE, fig.retina=2, "Distribution of Number of Issues Started per Month"}
subset(issues, !is.na(issue_start_date)) %>% 
  distinct(issue_id, issue_start_date = as.Date(floor_date(issue_start_date, 
                                                           "month"))) %>% 
  group_by(issue_start_date) %>% 
  summarise(score = n()) %>% 
  plot_frequency.month(x = issue_start_date, y = score,
                       y.breaks = seq(0, 185, 10),
                       labs.title = "Distribution of Number of Issues Started per Month", 
                       labs.x = "Issue start date (per month)")
  
```

### Issue Deadline Date

```{r echo=FALSE, fig.retina=2, "Distribution of Number of Issues Deadline per Month"}
subset(issues, !is.na(issue_deadline_date)) %>% 
  distinct(issue_id, issue_deadline_date = as.Date(floor_date(issue_deadline_date, 
                                                              "month"))) %>% 
  group_by(issue_deadline_date) %>% 
  summarise(score = n()) %>% 
  plot_frequency.month(x = issue_deadline_date, y = score,
                       y.breaks = seq(0, 80, 5),
                       labs.title = "Distribution of Number of Issues Deadline per Month", 
                       labs.x = "Issue deadline date (per month)")
  
```

### Issue Time Spent

```{r echo=FALSE, fig.retina=2, "Distribution of Time Spent per Issue"}
issues %>% 
  distinct(issue_id, issue_time_spent = period_to_seconds(issue_time_spent) / 3600) %>% 
  plot_histogram(x = issue_time_spent, binwidth = 1,
                 x.breaks = seq(0, 24, 1), y.breaks = seq(0, 3000, 150),
                 labs.title = "Distribution of Time Spent per Issue", 
                 labs.x = "Issue time spent (hours)") +
    coord_cartesian(xlim = c(0, 24))
  
```

### Issue Type

In the issue tracking system, 4 types of issues were used to classify the 
purpose of a issue. They are:

```{r echo=FALSE, message=FALSE, warning=FALSE, "Issue Type Values"}
levels(issues$issue_type)
```

The frequency of issues created by each issue type is shown in the graph below.

```{r echo=FALSE, fig.retina=2, "Frequency of Issue Type"}
issues %>%
  distinct(issue_id, issue_type) %>% 
  group_by(issue_type) %>% 
  summarise(score = n()) %>% 
  plot_frequency.factor(x = issue_type, y = score,
                        y.limits = c(0, .85),
                        labs.title = "Frequency of Issue Type",
                        labs.x = "Issue type")
```

Analysing the plot above, we note that most of the issues are of the MAINTENANCE 
type (around 75%). Next comes the issues of the CUSTOMIZATION type (around 20%). 
And, with less than 0.05% comes the issues of the types DATA MIGRATION and OTHERS.
Thus, we can summarize that project management was characterized by corrective 
maintenance activities of existing systems and subsystems.

The demand for adaptation and evolution of the systems was not insignificant. 
Although the amount of CUSTOMIZATION and DATA MIGRATION issues are less than 
MAINTENANCE, they may have indirectly influenced in the creation of large volume 
of MAINTENANCE issues. Maybe, CUSTOMIZATION issues had provocated related creation 
of MAINTENANCE issues. The same is true for DATA MIGRATION. An analysis of 
issues creation timeline by system/subsystem may answer that assumption.

### Issue System

```{r echo=FALSE, fig.retina=2, "Frequency of Issue System"}
issues %>%
  distinct(issue_id, issue_system) %>% 
  group_by(issue_system) %>% 
  summarise(score = n()) %>% 
  plot_frequency.factor(x = issue_system, y = score,
                        x.rotate = 20, y.limits = c(0, .85),
                        labs.title = "Frequency of Issue System",
                        labs.x = "Issue system")
```

### Issue Subsystem

```{r echo=FALSE, fig.retina=2, "Frequency of Issue Subsystem"}
issues %>%
  distinct(issue_id, issue_system, issue_subsystem) %>% 
  group_by(issue_system, issue_subsystem) %>% 
  summarise(score = n()) %>% 
  arrange(desc(score)) %>% 
  ungroup() %>% 
  filter(row_number() <= 15) %>% 
  plot_frequency.factor(x = paste0("(", issue_system, ") ", issue_subsystem), 
                        y = score, x.rotate = 25, y.limits = c(0, .45),
                        labs.title = "Frequency (Top 15) of Issue Subsystem",
                        labs.x = "Issue subsytem")
```

### Issue Stakeholder

```{r echo=FALSE, fig.retina=2, "Frequency of Issue Stakeholder"}
issues %>%
  distinct(issue_id, issue_stakeholder) %>% 
  group_by(issue_stakeholder) %>% 
  summarise(score = n()) %>% 
  plot_frequency.factor(x = issue_stakeholder, y = score,
                        y.limits = c(0, .75),
                        labs.title = "Frequency of Issue Stakeholder",
                        labs.x = "Issue stakeholder")
```

### Issue Created By

```{r echo=FALSE, fig.retina=2, "Frequency of Issue Created By"}
issues %>%
  distinct(issue_id, issue_stakeholder, issue_created_by) %>% 
  group_by(issue_stakeholder, issue_created_by) %>% 
  summarise(score = n()) %>% 
  arrange(desc(score)) %>% 
  ungroup() %>% 
  filter(row_number() <= 15) %>% 
  mutate(issue_created_by = first_last_names(issue_created_by)) %>% 
  plot_frequency.factor(x = paste0("(", issue_stakeholder, ") ", issue_created_by), 
                        y = score, x.rotate = 25, y.limits = c(0, .25),
                        labs.title = "Frequency (Top 15) of Issue Created By",
                        labs.x = "Issue created by")
```

### Issue Status

```{r echo=FALSE, fig.retina=2, "Frequency of Issue Status"}
issues %>%
  distinct(issue_id, issue_status) %>% 
  group_by(issue_status) %>% 
  summarise(score = n()) %>% 
  arrange(desc(score)) %>% 
  plot_frequency.factor(x = issue_status, y = score,
                        x.breaks = seq(0, 1, .1),
                        x.rotate = 20, y.limits = c(0, 1.1),
                        labs.title = "Frequency of Issue Status",
                        labs.x = "Issue status")
```

### Reflections on Data Set Summaries

What is the commom flow of the log states? (Data Set Structure)

What makes the demand low in 2013?
What makes the demand grow rapidly in 2014?



```{r echo=FALSE,  message=FALSE, warning=FALSE, "Clean the Environment"}
# rm(list = ls())
```