---
output: github_document
---
EDA in an Issue Tracking System Data Set
================================================================================
*by Dannyel Cardoso da Fonseca* 

```{r include=FALSE, "Global Set Up"}
# Cleaning the environment before start
# This is useful when execute "Restart R and Run All Chuncks"
rm(list = ls())

Sys.setlocale("LC_TIME","en_US.UTF-8")

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.path = "project_05_files/plots/", fig.retina = 3,
                      fig.height = 3.5, out.width = "86%")
```

```{r results="hide", "Load Packages"}
source("project_05.R")
load_packages(c("ggplot2", 
                "dplyr", 
                "lubridate"))
```

```{r "Project Set Up"}
# NOTE: To run this notebook set up current work directory to this file location

# Set up default theme for plots
theme_set(theme_classic() %+replace%
            theme(panel.grid.major.y = element_line(size=.05, color="grey50"),
                  plot.margin = unit(c(.3, 0, .5, 2), "cm"),
                  axis.text = element_text(size = rel(0.7), colour = "grey30"),
                  axis.title.x = element_text(size = rel(0.9),
                                              margin = margin(t = 5.5),
                                              vjust = 1),
                  axis.title.y = element_text(size = rel(0.9),
                                              angle = 90,
                                              margin = margin(r = 5.5), 
                                              vjust = 1),
                  plot.subtitle = element_text(size = rel(0.8), 
                                               hjust = 0, vjust = 1, 
                                               margin = margin(b = 5.5 * 0.9))))
```

```{r "Load Data Set"}
dataset <- load_dataset("issues_tracking.csv")

issues <- 
  dataset %>%
    select(starts_with("issue_")) %>%
    distinct() %>% 
    as.data.frame()

logs <- 
  dataset %>%
    select(issue_id, starts_with("log_")) %>%
    filter(!is.na(log_creation_date)) %>% 
    as.data.frame()
```

This project aims to explore a data set containing `r nrow(dataset)` observations 
of issues tracking and their history logs. The data set represents 5 years of project 
management which aimed maintenance and customization of many integrated systems
([SIG](https://docs.info.ufrn.br)) for the academic, administrative and human 
resources management at the Universidade Federal de Goi√°s ([UFG](https://www.ufg.br)).

A proprietary issue tracking system ([SIGProject](https://sigproject.esig.com.br)) 
was used to manage activities of company ([ESIG](https://www.esig.com.br)) and 
client ([Cercomp](https://www.cercomp.ufg.br)) teams.

![Screenshot](project_05_files/screenshot-sigproject.esig.com.br-2018.02.25-11-25-48.png)

I used data wrangling techniques to export data from that and clean them. The 
final data set, used in this project, and its documentation can be accessed, 
respectively, in these links: 

- [Issues Data Set](issues_tracking.csv)
- [Issues Data Set Documentation](issues_tracking.Rdoc.txt)

The exported data set, the wrangling process scripts and an example of one issue 
tracking can be find in [data_wrangling](data_wrangling) folder. 

# Initial Descriptive Analysis

In this section, we perform a univariate exploration of the data set, first 
examining its structure, followed by the analysis of each variable starting with 
issue variables and ending with log variables. In each group (issue and log) I 
begin with exploration of temporal variables and end with the analysis of 
categorical and range variables.

### Data Set Structure

The issues data set contains `r nrow(dataset)` rows and `r ncol(dataset)` 
variables. Of these 24 variables `r ncol(issues)` are about issue data and 
`r ncol(logs)-1` about issue's logs.

```{r "Structure of the Data Set"}
# Show the issues' dataframe structure
glimpse(dataset)
```

The number of distinct issues rows and issue's logs rows are 
`r nrow(issues)` and `r nrow(logs)` respectively. That is, the number of issues 
represents `r round(nrow(issues)*100/nrow(dataset))`% of the data set and the 
number of logs represents `r round(nrow(logs)*100/nrow(dataset))`% of it. The 
`r round((nrow(dataset)-nrow(logs))*100/nrow(dataset))`% of logs remaining 
(`r nrow(dataset)-nrow(logs)` rows) represents issues that do not have history 
logs. 

```{r}
df_number_logs_per_issue <- 
  dataset %>%
    select(issue_id, log_creation_date) %>%
    group_by(issue_id) %>% 
    summarise(score = sum(!is.na(log_creation_date)))
```

The distribution and the summaries of number of logs per issue are ploted in the
histogram below.

```{r "Distribution of the Number of Logs per Issue"}
df_number_logs_per_issue %>% 
  plot_distribution(score,
                    binwidth = 1,
                    title = "Distribution of the Number of Logs per Issue",
                    label.x = "Number of logs per issue",
                    breaks.x = seq(0, 43, 2),
                    breaks.y = seq(0, 800, 100))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

``` {r}
amount_upper_outiler <- 
  round(
    nrow(subset(df_number_logs_per_issue, score > 12)) * 100
    / 
    nrow(df_number_logs_per_issue)
  )
```

Analysing the plot above, we can note that most of issues have between 1 and 9 
history logs, approximately 1 standard deviation from the mean. Also, 75% of 
them have up to 6 history logs and 50% of issues have between 2 and 6 logs, a 
narrow range. Despite the skewed shape of plot, the median and mean are very 
close. This means that the amount of ouliers, after 12 logs per issue, is very 
low, approximately `r amount_upper_outiler`% of issues.

The representative narrow range of logs in an issue makes us think that there 
shoud be an activity flow pattern to resolve an issue. This flow pattern migth 
be observed in a commom sequence of log status. See more informations in the 
[Log Status](#log-status) section.

### Issue Creation Date

The data set analyzed comprises issues created in the period between 
`r format(min(issues$issue_creation_date), "%m/%d/%Y")` and 
`r format(max(issues$issue_creation_date), "%m/%d/%Y")`. More precisaly, between 
`r format(min(issues$issue_creation_date), "%H:%M:%S")` of 
`r format(min(issues$issue_creation_date), "%m/%d/%Y")` and 
`r format(max(issues$issue_creation_date), "%H:%M:%S")` of
`r format(max(issues$issue_creation_date), "%m/%d/%Y")`. 

```{r}
df_issue_created_by_month <- 
  issues %>% 
    filter(!is.na(issue_creation_date)) %>% 
    distinct(issue_id, 
             issue_creation_date = as.Date(floor_date(issue_creation_date,
                                                      "month")))
```

To get a sense of how it was the demand throughout the project see the plot 
below that shows the cumulative number of issues created per month along the 
project and its trend line.

```{r "Cumulative Number of Issues Created"}
df_issue_created_by_month %>% 
  plot_cumulative_ts(issue_creation_date,
                     title = "Cumulative Number of Issues Created per Month",
                     label.x = "Issue creation date (month)",
                     label.y = "Number of issues",
                     date_breaks.x = "3 month",
                     date_labels.x = "%b %Y",
                     breaks.y = seq(0, 4600, 500),
                     breaks.2nd.y = seq(0, 1, .1),
                     coord.ylim = c(0, 4600),
                     axis.text.x = element_text(angle = 40, hjust = 1))
```

> **Note:** In the plot above, the blue solid line represents the trend of the 
data and dotted grey vertical lines represent the 1st day of year.

We note that the first 11 months (from May 2013 to March 2014) of the project 
had very low demand, approximately 4% of the total. Some reasons for this were:

- The customer and company teams were small;
- There were no defined work processes between customer and company;
- That was the first time that the customer outsourced the development and 
deployment of systems;
- The customer's team had no experience with systems being deployed and the 
company's team did not know the customer's day-to-day business deeply. 

The work processes and knowledge on both sides (customer and company) were 
being built over those 11 months. The teams grew in size and new modules started
development and deployment. The next 12 months (from April 2014 to March 2015) 
there was significant growth curve in the activities, approximately 25% of
growth over the previous 11 months. From June 2015 the activities growth seems 
to stabilize, sometimes taking small growth curves. In this period occurred 
approximately 65% of project activities. Besides, the trend line fitted with 
growth curve demonstrating that the project began to have a standardized 
behavior.

In the area plot below, we can see the project's behavior in another way. This
plot shows the distribution of the number of issues created per month and
their cumulative summaries.

```{r "Distribution of the Number of Issues Created per Month"}
df_issue_created_by_month %>% 
  plot_ts(issue_creation_date,
          title = "Distribution of the Number of Issues Created per Month",
          label.x = "Issue creation date (month)",
          label.y = "Frequency",
          date_breaks.x = "3 month",
          date_labels.x = "%b %Y",
          breaks.y = seq(0, 195, 15),
          axis.text.x = element_text(angle = 40, hjust = 1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
accumulated quartile, the black and red solid lines represent, respectivaly, 
accumulated median and mean and dotted grey vertical lines represent the 1st day 
of year.

Analysing the area plot above, we note that the year 2013 had the lowest demand. 
The accumulated mean variates around 15 issues per month. At the end of 2013 and 
begin of 2014 there was a drop in activities because of the low administrative 
and academic activities at the university at the end of year. 

From April of 2014 until February of 2015 there was significant growth in 
activities. The accumulated mean increases from 15 to 50 issues per month and it 
overcame the accumulated median. The distance between accumulated 3rd quartile 
and accumulated median became greater than distance between accumulated 1st 
quartile and accumulated median. All this indicates that these months had a high 
number of issues created in relation to the past. 

But from August of 2015 the pace of growth dropped. Now, we can note that the 
distance between accumulated 1st quartile and accumulated median becomes greater 
than distance between accumulated 3rd quartile and accumulated median. This 
invertion provocated the overlap between accumulated mean and median. The 
stabilization of the growth also contributed to that. The average of growth pass 
to be only 10 monthly issues (from 70 to 80), approximately. 

We also note that except the significant growth period (from April 2014 to 
February 2015) the months between September and December of each year had a 
decline in activities. The reason is the low administrative and academic 
activities at the university in this period. Thus, the demand for customization 
and maintenance of the systems decreases.

```{r}
df_issue_created_by_wday <- 
  issues %>% 
    filter(!is.na(issue_creation_date)) %>% 
    distinct(issue_id, 
             issue_creation_wday = wday(issue_creation_date, label = TRUE))
```

During the week, the demand for new activities were concentrated on business days 
(from Monday to Friday) as we can note in the barplot below.

```{r "Frequency of Issues Created per Weekday"}
df_issue_created_by_wday %>% 
  plot_frequency(issue_creation_wday, 
                 title = "Frequency of Issues Created per Weekday",
                 label.x = "Weekday",
                 reorder.x = FALSE,
                 breaks.y = seq(0, 1020, 100),
                 breaks.2nd.y = seq(0, 1, .02))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean.

Note that whether we sort business days by the frequency of issue creations we 
have Tue > Wed > Thu > Fri > Mon. Tuesday is the weekday that has more issues 
creation (mode). Monday and Friday are the business days with less issues 
creation.  One of the reasons for this is that, from April 2014, the work began 
to be planned by sprints. These sprints were planned every Tuesday morning and 
issues were created in the afternoon.

```{r}
df_issue_created_by_weekend <-
  df_issue_created_by_wday %>% 
    filter(issue_creation_wday == "Sat" | issue_creation_wday == "Sun")
```

Issues created on weenkend (`r nrow(df_issue_created_by_weekend)` issues) may be 
considered outliers. These are issues created on weekend shift that was done 
during the enrollment periods. Theses outliers pull the mean and 1st quartile 
down and increase the standard deviationof mean. Removing them, we have a new 
barplot. See it below.

```{r "Frequency of Issues Created per Weekday (No Weekend)"}
df_issue_created_by_wday %>% 
  filter(!(issue_creation_wday %in% 
             df_issue_created_by_weekend$issue_creation_wday)) %>%
  plot_frequency(issue_creation_wday, 
                 title = "Frequency of Issues Created per Weekday",
                 subtitle_complement = "without the weekend",
                 label.x = "Weekday",
                 reorder.x = FALSE,
                 breaks.y = seq(0, 1020, 100),
                 breaks.2nd.y = seq(0, 1, .02))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean.

Note that the summaries are now approximated. The median and mean overlap, the 
distance between 1st and 3rd quartile became narrow and the standard deviation 
of mean descrease. These new summaries better reflect the behavior of issue 
creation.

```{r}
df_issue_created_by_hour <- 
  issues %>% 
    filter(!is.na(issue_creation_date)) %>%
    distinct(issue_id, 
             issue_creation_hour = hour(issue_creation_date))
```

Analysing how was the behaviour of the issue creations per hour of the day we 
have the bimodal distribution below.

```{r "Distribution of the Number of Issues Created per Hour of the Day"}
df_issue_created_by_hour %>% 
  plot_distribution(issue_creation_hour,
                    binwidth = 1,
                    title = "Distribution of the Number of Issues Created per Hour of the Day",
                    label.x = "Hour of the day",
                    breaks.x = seq(0, 23, 1),
                    breaks.y = seq(0, 700, 100),
                    limits.y = c(0, 700))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean,
the black dotted lines represent the lower and upper threshold to the outliers 
(lower = 1st qu. - 1.5 IQR, upper = 3rd qu. + 1.5 IQR).

The high volume of issue creation comprises between 8:00h and 18:00h, office 
hours. The peak hours are around 10:00h and 15:00h. At lunch time, the demand 
for new issues decreases. We note that this bimodal distribution is quasi 
symmetrical. The 1st and 3rd quartiles match with modes and the median and mean 
differ by a few minutes.

Ploting this distribution again but with only office hours (from 8:00h to 18:00h) 
we have the same summaries of the distribution before. See the plot below.

```{r "Distribution of the Number of Issues Created per Hour of the Day (Only Office Hours)"}
df_issue_created_by_hour %>%
  filter(issue_creation_hour >= 8 & issue_creation_hour <= 18) %>% 
  plot_distribution(issue_creation_hour,
                    binwidth = 1,
                    title = "Distribution of the Number of Issues Created per Hour of the Day",
                    subtitle_complement = "with only office hours",
                    label.x = "Hour of the day",
                    breaks.x = seq(0, 23, 1),
                    breaks.y = seq(0, 700, 100),
                    limits.y = c(0, 700))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean.

This shows us that the high issue creation volume actually comprises between 
8:00h and 18:00h.

### Issue Start Date

The behavior of the project in relation to the starting of a issue activity is 
similar to the behavior of its creation. The main differences are that the 
creation date has time information while start date does not have and there are 
more observation of creation date than start date.

```{r}
df_issue_start_date_month <- 
  issues %>% 
    filter(!is.na(issue_start_date)) %>% 
    distinct(issue_id, 
             issue_start_date = as.Date(floor_date(issue_start_date,
                                                   "month")))
```

To show how is similar the behavior between creation and start issue date see 
the cumulative number of issues started along the project and its trend line in 
the plot below.

```{r "Cumulative Number of Issues Started per Month"} 
df_issue_start_date_month %>% 
  plot_cumulative_ts(issue_start_date,
                     title = "Cumulative Number of Issues Started per Month",
                     label.x = "Issue start date (month)",
                     label.y = "Number of issues",
                     date_breaks.x = "3 month",
                     date_labels.x = "%b %Y",
                     breaks.y = seq(0, 4600, 500),
                     breaks.2nd.y = seq(0, 1, .1),
                     coord.ylim = c(0, 4600),
                     axis.text.x = element_text(angle = 40, hjust = 1))
```

> **Note:** In the plot above, the blue solid line represents the trend of the 
data and dotted grey vertical lines represent the 1st day of year.

Pratically it has the same shape and values of the Cumulative Number of Issues
Created per Month plot. 

This similarity induces us to believe that there was not a big delay between 
the creation and the starting of a issue. To verify this, I created a new 
variable named `issue_delay_start` which represents the delay, in calendar days, 
between creation of an issue and its start.

```{r}
issues$issue_delay_start <-
  issues %>% 
    select(issue_id, issue_creation_date, issue_start_date) %>% 
    mutate(issue_creation_date = as.Date(floor_date(issue_creation_date,
                                                    unit = "day"))) %>% 
    mutate(issue_delay_start = difftime(issue_start_date,
                                        issue_creation_date,
                                        units = "days")) %>% 
    pull(issue_delay_start) 
```

### Issue Delay Start

```{r}
df_issue_delay_start <-
  issues %>% 
    select(issue_id, issue_delay_start) %>% 
    filter(!is.na(issue_delay_start)) %>% 
    filter(issue_delay_start >= 0)
```

See in the plot below the distribution of the delay between creation and start 
date.

```{r "Distribution of the Delay Between Creation and Start Date"}
df_issue_delay_start %>%
  plot_distribution(issue_delay_start,
                    binwidth = 5,
                    title = "Distribution of the Delay Between Creation and Start Date",
                    label.x = "Number of calendar days",
                    breaks.x = seq(0, 500, 5),
                    coord.xlim = c(0, 70),
                    breaks.y = seq(0, 4000, 300),
                    limits.y = c(0, 4000))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted lines represents the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

```{r}
percent_delay_lte_5_days <- 
  round(
    nrow(subset(df_issue_delay_start, 
                issue_delay_start >=0 & issue_delay_start <= 5)) * 100
    /
    nrow(df_issue_delay_start)
  )
```

About `r percent_delay_lte_5_days`% of `r nrow(df_issue_delay_start)` issues, 
with start date information, have less than or equal 5 calendar days of delay 
to start. These amount of days is very short compared to the 30 days of the 
month (granularity used in the plots). 

### Issue Deadline Date

```{r}
na_deadline_date <- 
  round(
    nrow(subset(issues, is.na(issue_deadline_date))) * 100
    / 
    nrow(issues)
  )
```

Issue deadline date represents the planning date to deliver the issue. It is 
planned at the beginning (start) of the activities to solution an issue.

It seems to follow the same behaviour that issue creation and start date but it 
has some particularities, starting by amount of `NA` values. Issue deadline date 
has more `NA` values than those, approximately `r na_deadline_date`% of the 
number of issues. 

Its cumulative plot is also a bit different from the others. See the plot below.

```{r}
df_issue_deadline_date_month <- 
  issues %>% 
    filter(!is.na(issue_deadline_date)) %>% 
    distinct(issue_id, 
             issue_deadline_date = as.Date(floor_date(issue_deadline_date,
                                                      "month")))
```

```{r "Cumulative Number of Issues Deadlines per Month"}
df_issue_deadline_date_month %>% 
  plot_cumulative_ts(issue_deadline_date,
                     title = "Cumulative Number of Issues Deadlines per Month",
                     label.x = "Issue deadline date (month)",
                     label.y = "Number of issues",
                     date_breaks.x = "3 month",
                     date_labels.x = "%b %Y",
                     breaks.y = seq(0, 1600, 200),
                     breaks.2nd.y = seq(0, 1, .1),
                     coord.ylim = c(0, 1600),
                     axis.text.x = element_text(angle = 40, hjust = 1))
```

> **Note:** In the plot above, the blue solid line represents the trend of the 
data and dotted grey vertical lines represent the 1st day of year.

Until January of 2017, the growth curve of the cumulative number of issues 
deadlines per month is similiar to the creation and start growth curve. After 
that, a fall happens. Because that, the trend line does not fit well with growth 
curve.

In the plot below, we can see this behavior in another way.

```{r "Distribution of the Number of Issue Deadlines per Month"}
df_issue_deadline_date_month %>% 
  plot_ts(issue_deadline_date,
          title = "Distribution of the Number of Issue Deadlines per Month",
          label.x = "Issue deadline date (month)",
          label.y = "Frequency",
          date_breaks.x = "3 month",
          date_labels.x = "%b %Y",
          date_expand.x = c(0.02, 0),
          breaks.y = seq(0, 80, 5),
          axis.text.x = element_text(angle = 40, hjust = 1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
accumulated quartile, the black and red solid lines represent, respectivaly, 
accumulated median and mean and dotted grey vertical lines represent the 1st day 
of year.

Note how, after April of 2017, the frequency of issue deadlines decreases and 
does not rise again. The same occur with accumulated summaries. This phenomenon 
did not occur with creation and start date.

The reason for this is lack of data. After April of 2017, the number of issues 
with deadline information decreases. Few issues have planned your delivery date.
This is interesting information for teams to review in order to improve planning 
and control of activities.

```{r}
df_issue_deadline_wday <- 
  issues %>% 
    filter(!is.na(issue_deadline_date)) %>% 
    distinct(issue_id, 
             issue_deadline_wday = wday(issue_deadline_date, label = TRUE))
```

As expected, issue deadlines do not occur on Sundays. The deliver was planned to
business days (from Monday to Friday) as we can note in the barplot below.

```{r "Frequency of Issues Deadline per Weekday"}
df_issue_deadline_wday %>% 
  plot_frequency(issue_deadline_wday, 
                 title = "Frequency of Issues Deadline per Weekday",
                 label.x = "Weekday",
                 reorder.x = FALSE,
                 breaks.y = seq(0, 500, 50),
                 breaks.2nd.y = seq(0, 1, .02))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean,
and the black dotted lines represents the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

Issues with deadline to weenkend may be considered outliers. These are issues 
created on weekend shift (during the enrollment periods) with deadline size 0 
(day). Removing them, we have a new barplot. See it below.

```{r "Frequency of Issues Deadline per Weekday (No Weekend)"}
df_issue_deadline_wday %>% 
  filter(issue_deadline_wday != "Sat") %>%  
  plot_frequency(issue_deadline_wday, 
                 title = "Frequency of Issues Deadline per Weekday",
                 subtitle_complement = "without the weekend",
                 label.x = "Weekday",
                 reorder.x = FALSE,
                 breaks.y = seq(0, 500, 50),
                 breaks.2nd.y = seq(0, 1, .02))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean.

Tuesday is the weekday that has more issue deadlines (mode). Monday and Friday 
are the business days with less issue deadline. The reason for this is similar 
to issue creation and start date: from April 2014, the work began to be planned 
by sprints and these sprints were planned every Tuesday morning. The sprints had 
one week duration and, for a time, had two weeks duration. Always starting and 
ending on Tuesdays. It is worth mentioning that not all issues were part of the 
sprint and not all issues in the sprint had deadline of the sprint size.

To analize better this behavior I created a new variable named 
`issue_deadline_size` which represents the total planned time to solve and 
deliver an issue, that is, the difference between issue start and deadline date. 
This variable is interesting because it gives us an idea of the planning of the 
activities.

```{r}
issues$issue_deadline_size <-
  issues %>% 
    select(issue_id, issue_start_date, issue_deadline_date) %>%  
    mutate(issue_deadline_size = difftime(issue_deadline_date,
                                          issue_start_date,
                                          units = "days"))  %>% 
    pull(issue_deadline_size) 
```

### Issue Deadline Size

```{r}
df_issue_deadline_size <-
  issues %>% 
    select(issue_id, issue_deadline_size) %>% 
    filter(!is.na(issue_deadline_size)) %>% 
    filter(issue_deadline_size >= 0)
```

See in the plot below how is the distribution of the deadline size.

```{r "Distribution of the Deadline Size"}
df_issue_deadline_size %>%
  plot_distribution(issue_deadline_size,
                    binwidth = 1,
                    title = "Distribution of the Deadline Size",
                    label.x = "Number of calendar days",
                    breaks.x = seq(0, 45, 2),
                    coord.xlim = c(0, 45),
                    breaks.y = seq(0, 400, 30),
                    limits.y = c(0, 400))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represents upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

Note that the common planning is to solve issues and deliver them within 7 days 
or exactly in 14 days. The most issues are planned to deliver in the same day.
These issues should be easy to resolve or have high priority.

The reason for some issues have 14 days of deadline size is had a period in the 
project that there were two weeks sprints. But this planning was later 
changed to one week sprints.

### Issue Time Spent

```{r}
df_issue_time_spent_hour <- 
  issues %>% 
    filter(!is.na(issue_time_spent)) %>%
    distinct(issue_id, 
             issue_time_spent = (issue_time_spent / 3600))
```

Issue time spent is the total time spent solving the issue. It is a derivation 
of the sum of `log_time_spent`. In the dataset, issue time spent is represented 
in seconds. The plot below shows its distribution per hour.

```{r "Distribution of the Number of Time Spent on a Issue per Hour"}
df_issue_time_spent_hour %>%
  plot_distribution(issue_time_spent,
                    binwidth = 1,
                    title = "Distribution of the Number of Time Spent on a Issue per Hour",
                    label.x = "Hours spent",
                    breaks.x = seq(0, 110, 5),
                    breaks.y = seq(0, 3000, 250))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

Note as the distribution comprises below 5 hours (upper threshold to the 
outliers). The median is 30 minutes (0.5 hour). That is, 50% of time spent is 
less than 30 minutes. Also note that the mean and the 3rd quartile are almost 
the same, approximately 2 hours. This means the dispersion after 5 hours is 
insignicant in relation to the data mass less than 2 hours. Thus, lets zoom in
this plot to first 24 hours. See the result in the plot below.

```{r "Distribution of the Number of Time Spent on a Issue per Hour (Zoomed In)"}
df_issue_time_spent_hour %>%
  plot_distribution(issue_time_spent,
                    binwidth = 1,
                    title = "Distribution of the Number of Time Spent on a Issue per Hour",
                    subtitle_complement = "zoomed in to the first 24 hours",
                    label.x = "Hours spent",
                    breaks.x = seq(0, 24, 1),
                    coord.xlim = c(0, 24),
                    breaks.y = seq(0, 2700, 200))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

In the plot above, we can see better that time spent on an issue is usually less
than 1 hour. Thus, in the plot below we will analyze it on this scale.

```{r "Distribution of the Number of Time Spent on a Issue (Less Than 1 Hour)"}
df_issue_time_spent_1h <- 
  issues %>% 
    filter(!is.na(issue_time_spent)) %>%
    distinct(issue_id, issue_time_spent = (issue_time_spent / 60)) %>% 
    filter(issue_time_spent < 60)

df_issue_time_spent_1h %>%
  plot_distribution(issue_time_spent,
                    binwidth = 5,
                    title = "Distribution of the Number of Time Spent on a Issue per Hour",
                    subtitle_complement = "less than 1 hour",
                    label.x = "Minutes spent",
                    breaks.x = seq(0, 60, 5),
                    breaks.y = seq(0, 1500, 150))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

Note that the time spent is spread almost uniformly between 5 and 55 minutes and 
is well concentrated below 5 minutes. To better analyze the range between 0 and 
5 minutes I listed below the first 5 highest frequencies of the number of time 
spent on an issue.

```{r "Head Issue Time Spent"}
df_issue_time_spent_1h %>% 
  group_by(issue_time_spent) %>% 
  summarise(score = sum(!is.na(issue_time_spent))) %>% 
  arrange(desc(score)) %>% 
  head(5) %>% 
  as.data.frame()
```

Note that 1/3 of the issues spend 0 seconds. As there are no `NA` values in this 
variable, we can induce that the default behavior of the tool is to set 0 for 
issues that do not have time spent logged. Removing the 0s we have the following 
distribution.

```{r "Distribution of the Number of Time Spent on a Issue (Less Than 1 Hour and Non 0)"}
df_issue_time_spent_1h_non_0s <- 
  issues %>% 
    filter(!is.na(issue_time_spent)) %>%
    distinct(issue_id, issue_time_spent = (issue_time_spent / 60)) %>% 
    filter(issue_time_spent != 0 & issue_time_spent < 60)

df_issue_time_spent_1h_non_0s %>%
  plot_distribution(issue_time_spent,
                    binwidth = 6,
                    title = "Distribution of the Number of Time Spent on a Issue per Hour",
                    subtitle_complement = "less than 1 hour and non 0 seconds",
                    label.x = "Minutes spent",
                    breaks.x = seq(6, 60, 6),
                    breaks.y = seq(0, 500, 50))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean.

Now, notice that 75% of the issues, with the time spent less than an hour, took 
about 36 minutes to resolve. This time represents about 25% of the time spent 
when analyzing all issues. See this in the plot below that recreates the 
visualization of the time spent on an issue less than 24 hours.

```{r "Distribution of the Number of Time Spent on a Issue (Non 0)"}
df_issue_time_spent_non_0s <- 
  issues %>% 
    filter(!is.na(issue_time_spent)) %>%
    distinct(issue_id, issue_time_spent = (issue_time_spent / 3600)) %>% 
    filter(issue_time_spent != 0)

df_issue_time_spent_non_0s %>%
  plot_distribution(issue_time_spent,
                    binwidth = 1,
                    title = "Distribution of the Number of Time Spent on a Issue per Hour",
                    subtitle_complement = "non 0 seconds and zoomed in to the first 24 hours",
                    label.x = "Hours spent",
                    breaks.x = seq(0, 24, 1),
                    coord.xlim = c(0, 24),
                    breaks.y = seq(0, 1400, 150))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

Now, the mean and 75% of issues spend more or less 3 hours and 30 minutes to be 
resolved.

### Issue Type

In the issue tracking system, 65 issue types were used to classify the purpose 
of a issue. I grouped them in 4 issue types (see how in 
[issues_cleaning.R](data_wrangling/issues_cleaning.R), lines 54 to 126): 
CUSTOMIZATION, DATA MIGRATION, MAINTENANCE and OTHERS. Their frequencies of 
issues created are shown in the plot below.

```{r}
df_issue_type <- 
  issues %>% 
    filter(!is.na(issue_type)) %>% 
    distinct(issue_id, issue_type)
```

```{r "Frequency of Issue Type"}
df_issue_type %>% 
  plot_frequency(issue_type, 
                 title = "Frequency of Issue Type",
                 label.x = "Issue type",
                 breaks.y = seq(0, 3500, 250),
                 breaks.2nd.y = seq(0, 1, .1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean.

Analysing the plot above, we note that most of the issues are of the MAINTENANCE 
type (around 75%). Next comes the issues of the CUSTOMIZATION type (around 20%). 
And, around 5% comes the issues of the types DATA MIGRATION and OTHERS. Thus, we 
may summarize that project management was characterized by corrective maintenance 
activities on systems and subsystems.

The demand for adaptation and evolution of the systems was not insignificant. 
Although the amount of CUSTOMIZATION and DATA MIGRATION issues are less than 
MAINTENANCE, they may have indirectly influenced in the creation of large volume 
of MAINTENANCE issues. Maybe, CUSTOMIZATION issues had provocated related creation 
of MAINTENANCE issues. The same is true for DATA MIGRATION. An analysis of 
issue creations timeline by system/subsystem and issue types may answer that 
assumption. See [Issue Creation Date X Issue Subsystem X Issue Type](#issue-creation-date-x-issue-subsystem-x-issue-type) 
section.

### Issue System

On the integrated systems platform ([SIG](https://docs.info.ufrn.br)) there are 
3 main systems: SIGAA (academic), SIPAC (administrative) and SIGRH (human 
resources management). Within these systems, there are many subsystems (modules).
The other systems have purposes of support for these 3 systems or other 
activities.

```{r}
df_issue_system <- 
  issues %>% 
    filter(!is.na(issue_system)) %>% 
    distinct(issue_id, issue_system)
```

In the plot below, we may see the frequencies of issues created to 
`r length(unique(df_issue_system$issue_system))` systems.

```{r fig.height=4, "Frequency of Issue System"}
df_issue_system %>% 
  plot_frequency(issue_system, 
                 title = "Frequency of Issue System",
                 label.x = "Issue system",
                 breaks.y = seq(0, 3500, 250),
                 breaks.2nd.y = seq(0, 1, .1),
                 axis.text.x = element_text(angle = 20, hjust = 1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

Note that the academic system, SIGAA, surpasses all other systems together in 
number of created issues. It represents about 75% of all issues created. Also, 
note that, its frequency is considered an outlier (but in practice it is not).
SIPAC (administrative) and SIGRH (human resources management) follow as 2nd and 
3rd position. Both had roughly the same demand. Frequencies of the ohter systems 
are insignificant and we can consider them as outliers.

### Issue Subsystem

```{r}
df_issue_subsystem <- 
  issues %>% 
    filter(!is.na(issue_subsystem)) %>% 
    mutate(issue_subsystem = paste0("(", issue_system, ") ", issue_subsystem)) %>% 
    distinct(issue_id, issue_subsystem)
```

There are `r length(unique(df_issue_subsystem$issue_subsystem))` subsystems with 
at least one issue created. These subsystems belong to the 
`r length(unique(df_issue_system$issue_system))` systems listed in the previous 
section. In the plot below is shown the top 15 of the subsystems classified by the 
highest frequency of issue created.

```{r}
df_score_top15_issue_subsystem <- 
  df_issue_subsystem %>%
    group_by(issue_subsystem) %>% 
    summarise(score = n()) %>% 
    arrange(desc(score)) %>% 
    ungroup() %>% 
    filter(row_number() <= 15)
```

```{r fig.height=4.5, "Top 15 Frequency of Issue Subsystem"}
df_issue_subsystem %>% 
  filter(issue_subsystem %in% 
           df_score_top15_issue_subsystem$issue_subsystem) %>% 
  plot_frequency(issue_subsystem, 
                 title = "Top 15 Frequency of Issue Subsystem",
                 label.x = "Issue subsystem",
                 breaks.y = seq(0, 1400, 100),
                 breaks.2nd.y = seq(0, 1, .05),
                 axis.text.x = element_text(angle = 25, hjust = 1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

Above of the mean and out of the pattern are 3 academic subsystems: (SIGAA) 
GRADUA√á√ÉO, (SIGAA) STRICTO SENSU and SIGAA (GERAL). The subsystem (SIGAA) 
GRADUA√á√ÉO is visibly the most demanding subsystem. Wether the system SIGAA 
represents 75% of total demand and the subsystem (SIGAA) GRADUA√á√ÉO represents 
approximately 35% of it, then, the subsystem (SIGAA) GRADUA√á√ÉO represents 
approximately 1/4 of all demand of the project.

Also, note that the top 6 subsystems and 73% of top 15 belongs to academic 
system (SIGAA). This is natural because the main activity of a university is 
the academy.

### Issue Stakeholder

On the project, there are two stakeholder types that open issues in the issue 
tracking system ([SIGProject](https://sigproject.esig.com.br)): the IT team of
Universidade Federal de Goi√°s ([UFG](https://www.ufg.br)) and the IT team of the 
company ([ESIG](https://www.esig.com.br)) that was hired to support the systems 
maintenance and customization. Here, I labeled the IT team of UFG as CUSTOMER 
and the IT team of the ESIG as COMPANY. The plot below shows how was the 
frequency of open issues in the facet of stakeholders.

```{r}
df_issue_stakeholder <- 
  issues %>% 
    filter(!is.na(issue_stakeholder)) %>% 
    distinct(issue_id, issue_stakeholder)
```

```{r "Frequency of Issue Stakeholder"}
df_issue_stakeholder %>% 
  plot_frequency(issue_stakeholder, 
                 title = "Frequency of Issue Stakeholder",
                 label.x = "Issue stakeholder",
                 breaks.y = seq(0, 3500, 250),
                 breaks.2nd.y = seq(0, 1, .1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean.

We note that the CUSTOMER created twice more issues than COMPANY. This is make
sense since it is the customer who has needs and, hence, it opens demands. The 
COMPANY also created issues, but its is to control internally activities that 
are need to solution the issue created by CUSTOMER. But seeing the plot we note
that there is no a relationship 1:1 between the issue created by CUSTOMER and 
the issue created by COMPANY. This is because many activities of the issues 
created by CUSTOMER were managed in the issue itself.

### Issue Created By

```{r}
df_issue_stakeholder_creator <- 
  issues %>% 
    filter(!is.na(issue_created_by)) %>% 
    distinct(issue_stakeholder, issue_created_by)
```

As discussed in the previous section, CUSTOMER has created twice more issues 
than COMPANY, but seeing the plot below we noticed that the CUSTOMER did this 
with a team half the size of the COMPANY team. The proportion of people in each 
team is inversely proportional to the demand generated by them. 

The plot below shows the number of distinct users by stakeholder who have 
created at least one issue related to the project.

```{r "Frequency of Distinct Users by Stakeholder"}
df_issue_stakeholder_creator %>% 
  plot_frequency(issue_stakeholder, 
                 title = "Frequency of Distinct Users by Stakeholder",
                 label.x = "Stakeholder",
                 breaks.y = seq(0, 150, 10),
                 breaks.2nd.y = seq(0, 1, .1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean.

There are `r nrow(df_issue_stakeholder_creator)` users in
issue tracking system who have created at least one issue related to the project. 
These users, 
`r sum(df_issue_stakeholder_creator$issue_stakeholder == "COMPANY")` 
belong to the COMPANY part and 
`r sum(df_issue_stakeholder_creator$issue_stakeholder == "CUSTOMER")` 
belong to the CUSTOMER part. This proportion makes sense since the CUSTOMER team
is composed by Business Process Analyst/Designer and Tester roles and the 
COMPANY team is composed by Product Owner, Requirement Analyst/Designer, 
Software Architect, Developer and Tester roles. But as the demand comes from the 
CUSTOMER, the volume of issues opened by it is much more than opened by COMPANY.

```{r}
df_issue_created_by <- 
  issues %>% 
    filter(!is.na(issue_created_by)) %>% 
    mutate(issue_created_by = paste0("(", issue_stakeholder, ") ", 
                                     first_last_names(issue_created_by))) %>% 
    distinct(issue_id, issue_created_by)
```

Now, analysing the plot below that shows the top 15 frequency of issues opened 
by user we note that the 1st top user is from COMPANY following by 7 users 
from CUSTOMER.

```{r}
df_score_top15_issue_created_by <- 
  df_issue_created_by %>%
    group_by(issue_created_by) %>% 
    summarise(score = n()) %>% 
    arrange(desc(score)) %>% 
    ungroup() %>% 
    filter(row_number() <= 15)
```

```{r fig.height=4.5, "Top 15 Frequency of Issue Created By"}
df_issue_created_by %>% 
  filter(issue_created_by %in% 
           df_score_top15_issue_created_by$issue_created_by) %>%
  plot_frequency(issue_created_by, 
                 title = "Top 15 Frequency of Issue by Creator",
                 label.x = "Issue creator",
                 breaks.y = seq(0, 1400, 100),
                 breaks.2nd.y = seq(0, 1, .025),
                 axis.text.x = element_text(angle = 20, hjust = 1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

The 1st top issue creator, RAMON RODRIGUES, was the Product Owner of the 
COMPANY in the project in question. He was the main contact in the COMPANY. Many 
issues created by him were reported by CUSTOMER (through Skype) and he created 
many issues to distribute activities to the COMPANY team. Throughout the project, 
he created 20% of all issues.

The 2nd and 3rd top issue creator are Testers in CUSTOMER team. They created 
issues for each bug found in the systems.

The others 5 CUSTOMER users are Business Process Analyst/Designer of the academic
systems.

### Issue Status

```{r}
df_issue_status <- 
  issues %>% 
    filter(!is.na(issue_status)) %>% 
    distinct(issue_id, issue_status)
```

Issue status represents the current status of an issue. See in the plot below 
the current status of all issues.

```{r fig.height=4, "Frequency of Issue Status"}
df_issue_status %>% 
  plot_frequency(issue_status, 
                 title = "Frequency of Issue Status",
                 label.x = "Issue status",
                 breaks.y = seq(0, 4500, 500),
                 breaks.2nd.y = seq(0, 1, .1),
                 axis.text.x = element_text(angle = 20, hjust = 1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

As we can notice, 
`r round(sum(df_issue_status$issue_status == "FINISHED")*100/nrow(df_issue_status), 1)`% 
of issues are with FINISHED status. This makes sense because this data set 
comprises 5 years of project management and it is obvious that the most issues 
are FINISHED. 

### Issue Priority Number

```{r}
df_issue_priority_number <- 
  issues %>% 
    filter(!is.na(issue_priority_number)) %>%
    distinct(issue_id, issue_priority_number)
```

When creating an issue, it is given a priority number between 0 (highest) and 
999 (default value, equivalent to null). The plot below shows the distribution 
of the priority number among issues.

```{r "Distribution of the Priority Number per Issue"}
df_issue_priority_number %>%
  plot_distribution(issue_priority_number,
                    binwidth = 50,
                    title = "Distribution of the Priority Number per Issue",
                    label.x = "Issue priority number",
                    breaks.x = seq(0, 999, 50),
                    breaks.y = seq(0, 2700, 200))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean.

Note that the issues are distributed into two ends: one between 0-50 and one 
between 950-999. Also note that the 1st quartile is very close to the lowest 
priority number (0) and the median and 3rd quartile is equal to the highest 
priority number (999). This scenario means either priority numbers are next to 
1st quartile threshold or 999 are assigned to priority numbers. See this in 
another way in the table below that list the 6 highest issue priority numbers.

```{r "Top 6 Issue Priority Number"}
df_issue_priority_number %>% 
  group_by(issue_priority_number) %>% 
  summarise(score = sum(!is.na(issue_priority_number))) %>% 
  arrange(desc(score)) %>% 
  head() %>% 
  as.data.frame()
```

As 999 is the default value in the tool and is equivalent to null, I removed it
from the data set and I generated another plot showing the distribution of 
priority number less than 100. See it below.

```{r "Distribution of the Priority Number per Issue (Dropped 999 and Zoomed In to 100)"}
df_issue_priority_number_dropped_999 <- 
  df_issue_priority_number %>% 
    filter(issue_priority_number != 999)

df_issue_priority_number_dropped_999 %>%
  plot_distribution(issue_priority_number,
                    binwidth = 10,
                    title = "Distribution of the Priority Number per Issue",
                    subtitle_complement = "after dropped 999s",
                    label.x = "Issue priority number",
                    breaks.x = seq(0, 280, 10),
                    coord.xlim = c(0, 100),
                    breaks.y = seq(0, 1600, 150))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

Now, we can see the median with 4 value and the 3rd quartile with 12. Also, note
that priority numbers greather than 28 are outliers. Thus, to analyse better the
behaviour of priority numbers I have generated another plot zooming into 
priority numbers less than or equal to 30.

```{r "Distribution of the Priority Number per Issue (Less than or Equal to 30)"}
df_issue_priority_number_dropped_999 %>%
  plot_distribution(issue_priority_number,
                    binwidth = 1,
                    title = "Distribution of the Priority Number per Issue",
                    subtitle_complement = "less than or equal to 30",
                    label.x = "Issue priority number",
                    breaks.x = seq(0, 30, 1),
                    coord.xlim = c(0, 30),
                    breaks.y = seq(0, 450, 50),
                    coord.ylim = c(0, 450))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

Analysing the plot above, we can realize that 50% of priority numbers assigned
follow the [Likert scale](https://en.wikipedia.org/wiki/Likert_scale): 0 
(highest) to 4 (lowest). The scale 0 (highest) to 9 (lowest) are commonly used
too.

### Issue Priority Scale

```{r}
df_issue_priority_scale <-
  issues %>% 
    filter(!is.na(issue_priority_scale)) %>%
    distinct(issue_id, issue_priority_scale)
```

The issue priority scale is another way to assign a priority. But instead of 
assign numbers it is assigned ordinal categories. The issue tracking system 
define 6 priority scale from lowest to highest: SUSPENDED, LOW, MEDIUM, HIGH, 
URGENT, BLOCKING. The priority scale HIGH is the default.

The plot below shows the frequency of each priority scale.

```{r "Frequency of Issue Priority Scale"}
df_issue_priority_scale %>% 
  plot_frequency(issue_priority_scale, 
                 title = "Frequency of Issue Priority Scale",
                 label.x = "Issue priority scale",
                 reorder.x = FALSE,
                 breaks.y = seq(0, 4500, 300),
                 breaks.2nd.y = seq(0, 1, .1))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

The HIGH priority is the most assigned. This is occur because it is the default
priority and was defined between teams (CUSTOMER and COMPANY) that priority 
scale change only would occur if necessary. LOW and MEDIUM priority would be 
assigned to CUSTOMIZATION issues, not MAINTENANCE. And to MAINTENANCE issues 
HIGH (default), URGENT and BLOCKING priorities would be assigned.

### Issue Progress

```{r}
df_issue_progress <- 
  issues %>% 
    filter(!is.na(issue_progress)) %>% 
    distinct(issue_id, issue_progress)
```

Issue progress represents the current progress of an issue. As in 
[Issue Status](#issue-status) the most of issues are with FINISHED, that is, 
100% of progress. See this in the plot below.

```{r "Distribution of the Progress per Issue"}
df_issue_progress %>%
  plot_distribution(issue_progress,
                    binwidth = 10,
                    title = "Distribution of the Progress per Issue",
                    label.x = "Issue progress",
                    breaks.x = seq(0, 100, 10),
                    breaks.y = seq(0, 4600, 500))
```

> **Note:** In the plot above, the black dashed lines represent the 1st and 3rd 
quartile, the black and red solid lines represent, respectivaly, median and mean
and the black dotted line represent the upper threshold to the outliers 
(3rd qu. + 1.5 IQR).

From here, I start the analysis of issue logs.

### Log Build Info

```{r}
df_log_build_info <-
  logs %>% 
    filter(!is.na(log_build_info)) %>% 
    distinct(issue_id, log_build_info) %>% 
    group_by(log_build_info) %>% 
    summarise(score = n()) %>% 
    arrange(desc(score)) 
```

```{r "Distribution of the Number of the Issues per System's Version"}
df_log_build_info %>%
  plot_distribution(score,
                    binwidth = 5,
                    title = "Distribution of the Number of the Issues per System's Version",
                    label.x = "Number of issues",
                    breaks.x = seq(1, 125, 5),
                    breaks.y = seq(0, 320, 30))
```

```{r "Distribution of the Number of the Issues per System's Version (Less Than 6 Issues)"}
df_log_build_info <- 
  logs %>% 
    filter(!is.na(log_build_info)) %>% 
    distinct(issue_id, log_build_info) %>% 
    group_by(log_build_info) %>% 
    summarise(score = n()) %>% 
    arrange(desc(score)) %>% 
    filter(score < 6)

df_log_build_info %>%
  plot_distribution(score,
                    binwidth = 1,
                    title = "Distribution of the Number of the Issues per System's Version",
                    subtitle_complement = "less than 6 issues",
                    label.x = "Number of issues",
                    breaks.x = seq(1, 5, 1),
                    breaks.y = seq(0, 90, 10))
```

### Log Status

> **Go back:** [Data Set Structure](#data-set-structure) 

### Reflections on Data Set Summaries

What is the commom flow of the log states? (Data Set Structure)

What makes the demand low in 2013?
What makes the demand grow rapidly in 2014?

### Association Between Variables

#### Issue Creation Date X Issue Subsystem X Issue Type

An analysis of issue creations timeline by system/subsystem and issue types may 
answer that assumption.